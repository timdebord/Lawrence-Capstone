---
title: "Week 4 Progress Report"
output: html_document
date: "2025-01-31"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
library(tidyverse)
library(dplyr)
library(stargazer)
library(knitr)
library(kableExtra)
library(plm)
library(lme4)
library(sf)
library(leaflet)
library(spdep)
library(spatialreg)
library(sp)
library(terra)
library(tmap)
library(splm)
library(plm)
library(corrplot)
library(gridExtra)
library(car)
library(lmtest)
```

# Loading in Data

```{r}
df <- read.csv("C:/Users/timmy/Desktop/School Files/Capstone/Final CSVs/Final Spatial Data Natural Hazards.csv")
states_sf <- st_read("C:/Users/timmy/Desktop/School Files/Capstone/tl_2024_us_state.shp", promote_to_multi = FALSE)

df$State <- as.factor(df$State)
```

```{r}
#states_sf <- st_cast(states_sf, "POLYGON")

states_sf <- states_sf %>%
  rename(State = NAME)

# Temporarily save geometry and attributes separately
geometry <- st_geometry(states_sf)
attributes <- st_drop_geometry(states_sf)

# Expand attributes to include all years
expanded_attributes <- attributes %>%
  mutate(dummy = 1) %>%  
  expand_grid(Year = 2008:2019) %>%  
  select(-dummy)  

# Combine expanded attributes with original geometry
states_sf <- st_sf(expanded_attributes, geometry = rep(geometry, each = length(2008:2019)))
```

# Data Wrangling

```{r}
df_test <- df 
#df_test$Percent_white <- df_test$Percent_white * 100

#df_test$DAMAGE_PROPERTY <- df_test$DAMAGE_PROPERTY / 1000
#df_test$DAMAGE_CROPS <- df_test$DAMAGE_CROPS / 1000
```

```{r}
final_df <- states_sf %>%
  inner_join(df_test, by = c("State", "Year"))
```

```{r}
final_df <- final_df %>%
  filter(!State %in% c("Hawaii", "Alaska"))

final_df <- final_df %>%
  select(-total_damage_property.y, -total_damage_crops.y) %>%  
  rename(
    total_damage_property = total_damage_property.x,  
    total_damage_crops = total_damage_crops.x  
  )

#model_ready <- final_df %>%
  #arrange(Year, State)  %>% 
  #mutate(total_damage = total_damage_property + total_damage_crops) %>% 
  #mutate(td_per_cap = total_damage / Total_Residents)

model_ready <- final_df %>%
  arrange(Year, State)  %>% 
  mutate(total_damage = total_damage_property) %>% 
  mutate(td_per_cap = total_damage / Total_Residents)

model_ready <- model_ready %>%
  rename(
    homePrems = Home.Avg.Premium,
    rentersPrems = Renters.Avg.Premium,
    HealthcareExp = Health.Spending.per.Capita,
    Pop18_64 = Total_Pop_Dis,
    Uninsured = Percent.Adults.19.64.Uninsured,
    Medicaid = Percent,
    PriceDef = RegionalPriDef,
    Coastal = Is_Coastal,
    MedHomeIncome = Median.Income,
    AvgHomePrice = Avg_Home_Price,
    RiskIndex = National_Risk_Index_Score_Composite,
    ExpAnnualLoss = Expected.Annual.Loss...Score...Composite
  )
```

I redid my damage variable because, honestly, my previous method was pretty lazy. This time, I separated damage by type (property vs. crops) and by storm event. This ensures the model has access to the correct damage numbers. Additionally, based on this research paper https://www.nber.org/system/files/working_papers/w32579/w32579.pdf, I decided to split the damage into categories: those covered by typical homeowners insurance and those requiring separate coverage (earthquake, flood, etc). This helps the model distinguish between what is and isn’t covered by standard homeowners insurance.

```{r}
geometry <- st_geometry(model_ready)  
model_ready_no_geom <- model_ready %>%
  st_drop_geometry()

sev_weather_property_cols <- names(model_ready_no_geom) %>%
  str_subset("damage_property_(tornado|excessive_heat|heavy_snow|high_wind|hail|winter_storm|blizzard|ice_storm|strong_wind|lightning)")

sev_weather_crops_cols <- names(model_ready_no_geom) %>%
  str_subset("damage_crops_(tornado|excessive_heat|heavy_snow|high_wind|hail|winter_storm|blizzard|ice_storm|strong_wind|lightning)")

disaster_property_cols <- names(model_ready_no_geom) %>%
  str_subset("damage_property_(hurricane|flood|flash_flood|storm_surge_tide|wildfire|coastal_flood|lakeshore_flood|tsunami)")

disaster_crops_cols <- names(model_ready_no_geom) %>%
  str_subset("damage_crops_(hurricane|flood|flash_flood|storm_surge_tide|wildfire|coastal_flood|lakeshore_flood|tsunami)")

model_ready_no_geom <- model_ready_no_geom %>%
  mutate(
    num_sev_weather_property = rowSums(select(., all_of(sev_weather_property_cols)), na.rm = TRUE),
    num_sev_weather_crops = rowSums(select(., all_of(sev_weather_crops_cols)), na.rm = TRUE),
    disasters_property = rowSums(select(., all_of(disaster_property_cols)), na.rm = TRUE),
    disasters_crops = rowSums(select(., all_of(disaster_crops_cols)), na.rm = TRUE)
  ) %>%
  mutate(
    home_damage = num_sev_weather_property + num_sev_weather_crops,
    disaster_damage = disasters_property + disasters_crops,
    combined_damage = home_damage + disaster_damage + 2000,
    climate_index = combined_damage / RiskIndex,
    building_risk = (ExpAnnualLoss / Total_Value) * PriceDef,
    realized_damaged = (combined_damage / ExpAnnualLoss) * RiskIndex,
    actual_NRI = (combined_damage) * (Social/ Community)
  )

model_ready <- st_sf(model_ready_no_geom, geometry = geometry)
```

$$
\left( \frac{\text{Expected Annual Loss in 2024}}{\text{Total Building and Agri Value in 2024}} \right) \times \text{Price Deflator} = \text{Perceived Risk in 2024 Adjusted for the current years Price Deflator}
$$

$$
\left( \frac{\text{Total Damage}}{\text{Expected Annual Loss in 2024}} \right) \times \text{National Risk Index in 2024} = \text{Ratio of the Realized Damage adjusted by State National Risk Score}
$$

$$
\left( \frac{\text{Total Combined Damage}}{\text{National Risk Index in 2024}} \right)  = \text{Ratio of Total Damage and the National Risk Index}
$$

```{r}
states <- c("Alabama", "Alaska", "Arizona", "Arkansas", "California", "Colorado", "Connecticut", 
            "Delaware", "Florida", "Georgia", "Hawaii", "Idaho", "Illinois", "Indiana", 
            "Iowa", "Kansas", "Kentucky", "Louisiana", "Maine", "Maryland", 
            "Massachusetts", "Michigan", "Minnesota", "Mississippi", "Missouri", "Montana", 
            "Nebraska", "Nevada", "New Hampshire", "New Jersey", "New Mexico", "New York", 
            "North Carolina", "North Dakota", "Ohio", "Oklahoma", "Oregon", "Pennsylvania", 
            "Rhode Island", "South Carolina", "South Dakota", "Tennessee", "Texas", 
            "Utah", "Vermont", "Virginia", "Washington", "West Virginia", "Wisconsin", 
            "Wyoming")

square_miles <- c(52420, 665384, 113990, 53179, 163695, 104094, 5543, 2489, 65758, 59425, 10932, 83569, 
                  57914, 36420, 56273, 82278, 40408, 52378, 35380, 12406, 10554, 96714, 86936, 48432, 
                  69707, 147040, 77348, 110572, 9349, 8723, 121590, 54555, 53819, 70698, 44826, 
                  69899, 98379, 46054, 1545, 32020, 77116, 42144, 268596, 84897, 9616, 42775, 71298, 
                  24230, 65496, 97813) # Census Bureau 

years <- rep(2008:2019, each = length(states))

state_area <- data.frame(State = rep(states, times = length(2008:2019)),
                       Square_Miles = rep(square_miles, times = length(2008:2019)),
                       Year = years)

state_area <- state_area[!state_area$State %in% c("Alaska", "Hawaii"), ]

model_ready <- model_ready %>% 
  left_join(state_area, by = c("State", "Year"))

model_ready$PopDensity <- model_ready$Total_Residents / model_ready$Square_Miles

model_ready <- model_ready %>%
  mutate(Region = case_when(
    State %in% c("Minnesota", "Iowa", "Missouri", "Wisconsin", "Indiana", "Illinois", "Michigan", "Ohio") ~ "Midwest",
    State %in% c("Washington", "Oregon", "Idaho") ~ "Northwest",
    State %in% c("Colorado", "Kansas", "Montana", "North Dakota", "Nebraska", "South Dakota", "Wyoming") ~ "North Central",
    State %in% c("Connecticut", "Delaware", "Kentucky", "Massachusetts", "Maryland", "Maine", "New Hampshire", "New Jersey", "New York", "Pennsylvania", "Rhode Island", "Virginia", "Vermont", "West Virginia") ~ "Northeast",
    State %in% c("Arizona", "Nevada", "Utah", "California") ~ "Southwest",
    State %in% c("Oklahoma", "Texas", "New Mexico", "Louisiana") ~ "South Central",
    State %in% c("North Carolina", "South Carolina", "Georgia", "Alabama", "Mississippi", "Florida", "Tennessee", "Arkansas") ~ "Southeast",
    TRUE ~ "Unknown"  # Default case for missing or unmatched states
  ))

model_ready <- model_ready %>%
  arrange(Year, State)

### STATE-YEAR FIXED EFFECTS ###
model_ready <- model_ready %>% 
  mutate(RegionYearFE = paste(Region,Year, sep= "-"))
```

```{r}
model_ready_scaled <- model_ready %>%
  mutate(across(c(homePrems, PriceDef, disaster_damage, PopDensity, td_per_cap, MedHomeIncome, home_damage, Percent_white, AvgHomePrice, ExpAnnualLoss, RiskIndex, Total_Buidling_Value, combined_damage, climate_index, building_risk, realized_damaged, actual_NRI), 
                ~ {
                  scaled_value <- (.-min(.)) / (max(.) - min(.))
                  scaled_value[scaled_value == 0] <- 0.00000000000001
                  scaled_value
                }))
```

# Neighors and Weights

Based on my intuition, a dynamic weight system seems most appropriate for capturing the effects of storms on homeowners' premiums. Initially, I used a queen contiguity approach with equal weighting for all neighbors, but that didn't seem suitable. A massive cold storm will likely affect the entire Northeast, but not evenly. So, I switched to distance-based neighbors, setting a 720 km threshold to ensure Texas and Louisiana were considered neighbors. Looking at states like New York and Alabama, the assigned neighbors seemed reasonable. The only downside is that California ends up with just one neighbor, but I think the benefits of properly fitting the Northeast outweigh that drawback.


```{r}
nb <- model_ready %>%
  filter(Year %in% c(2011))

centroids <- st_centroid(nb)  # This should already be in 'sf' format

centroids_sp <- as(centroids, "Spatial")

dist_matrix <- spDists(centroids_sp)
# First, create a circle around each centroid
circle_radius <- 525  # Your specified distance threshold
buffer <- st_buffer(centroids, dist = circle_radius)

neighbors <- dnearneigh(centroids_sp, 0, 525)

# Plot everything in the correct order
plot(st_geometry(nb), border = "black")  # Original polygons
plot(buffer, add = TRUE, col = "transparent", border = "red", lty = 2, density = 50)  # Distance circles
plot(neighbors, st_coordinates(st_centroid(st_geometry(nb))), add = TRUE, 
     col = "blue", lwd = 1.5)  # Neighbor connections

```


```{r}
nb <- model_ready %>%
  filter(Year %in% c(2011))

centroids <- st_centroid(nb)  # This should already be in 'sf' format

centroids_sp <- as(centroids, "Spatial")

dist_matrix <- spDists(centroids_sp)

# Create neighbors list
neighbors <- dnearneigh(centroids_sp, 0, 525)

# Define row indices for California, Arizona, and Oregon
california_index <- 4L  # Ensure it's an integer
arizona_index <- 2L
oregon_index <- 35L

neighbors[[california_index]] <- sort(unique(as.integer(c(neighbors[[california_index]], arizona_index, oregon_index))))
neighbors[[arizona_index]] <- sort(unique(as.integer(c(neighbors[[arizona_index]], california_index))))
neighbors[[oregon_index]] <- sort(unique(as.integer(c(neighbors[[oregon_index]], california_index))))

# Define row indices for Florida and its neighboring states
florida_index <- 8L  # Replace X with Florida's row index
mississippi_index <- 22L  # Replace Y with Mississippi's row index
alabama_index <- 1L   # Replace Z with Alabama's row index
south_carolina_index <- 38L  # Replace A with South Carolina's row index
louisiana_index <- 16L  # Replace B with Louisiana's row index

# Update the neighbors list
neighbors[[florida_index]] <- sort(unique(as.integer(c(neighbors[[florida_index]], mississippi_index, alabama_index, south_carolina_index, louisiana_index))))
neighbors[[mississippi_index]] <- sort(unique(as.integer(c(neighbors[[mississippi_index]], florida_index))))
neighbors[[alabama_index]] <- sort(unique(as.integer(c(neighbors[[alabama_index]], florida_index))))
neighbors[[south_carolina_index]] <- sort(unique(as.integer(c(neighbors[[south_carolina_index]], florida_index))))
neighbors[[louisiana_index]] <- sort(unique(as.integer(c(neighbors[[louisiana_index]], florida_index))))

# Define row indices for Texas and its neighboring states
texas_index <- 41L  # Replace X with Texas's row index
new_mexico_index <- 29L  # Replace Y with New Mexico's row index
arkansas_index <- 3L  # Replace A with Arkansas's row index

# Update the neighbors list
neighbors[[texas_index]] <- sort(unique(as.integer(c(neighbors[[texas_index]], new_mexico_index, louisiana_index, arkansas_index))))
neighbors[[new_mexico_index]] <- sort(unique(as.integer(c(neighbors[[new_mexico_index]], texas_index))))
neighbors[[louisiana_index]] <- sort(unique(as.integer(c(neighbors[[louisiana_index]], texas_index))))
neighbors[[arkansas_index]] <- sort(unique(as.integer(c(neighbors[[arkansas_index]], texas_index))))

# Define row indices for Colorado, Kansas, and Nebraska
colorado_index <- 5L  # Replace X with Colorado's row index
kansas_index <- 14L  # Replace Y with Kansas's row index
nebraska_index <- 25L  # Replace Z with Nebraska's row index

# Update the neighbors list
neighbors[[colorado_index]] <- sort(unique(as.integer(c(neighbors[[colorado_index]], kansas_index, nebraska_index))))
neighbors[[kansas_index]] <- sort(unique(as.integer(c(neighbors[[kansas_index]], colorado_index))))
neighbors[[nebraska_index]] <- sort(unique(as.integer(c(neighbors[[nebraska_index]], colorado_index))))

# Define row indices for Idaho, Utah, and Wyoming
idaho_index <- 10L  # Replace X with Idaho's row index
utah_index <- 42L  # Replace Y with Utah's row index
wyoming_index <- 48L  # Replace Z with Wyoming's row index

# Update the neighbors list
neighbors[[idaho_index]] <- sort(unique(as.integer(c(neighbors[[idaho_index]], utah_index, wyoming_index))))
neighbors[[utah_index]] <- sort(unique(as.integer(c(neighbors[[utah_index]], idaho_index, wyoming_index))))
neighbors[[wyoming_index]] <- sort(unique(as.integer(c(neighbors[[wyoming_index]], idaho_index, utah_index))))


# Update the neighbors list
neighbors[[nebraska_index]] <- sort(unique(as.integer(c(neighbors[[nebraska_index]], kansas_index, wyoming_index))))
neighbors[[kansas_index]] <- sort(unique(as.integer(c(neighbors[[kansas_index]], nebraska_index))))
neighbors[[wyoming_index]] <- sort(unique(as.integer(c(neighbors[[wyoming_index]], nebraska_index))))


# Define row indices for Washington and Idaho
washington_index <- 45L  # Replace X with Washington's row index

# Update the neighbors list
neighbors[[washington_index]] <- sort(unique(as.integer(c(neighbors[[washington_index]], idaho_index))))
neighbors[[idaho_index]] <- sort(unique(as.integer(c(neighbors[[idaho_index]], washington_index))))

# Define row indices for Nevada and Idaho
nevada_index <- 26L  # Replace X with Nevada's row index

# Update the neighbors list
neighbors[[nevada_index]] <- sort(unique(as.integer(c(neighbors[[nevada_index]], idaho_index))))
neighbors[[idaho_index]] <- sort(unique(as.integer(c(neighbors[[idaho_index]], nevada_index))))

# Update the neighbors list
neighbors[[new_mexico_index]] <- sort(unique(as.integer(c(neighbors[[new_mexico_index]], arizona_index))))
neighbors[[arizona_index]] <- sort(unique(as.integer(c(neighbors[[arizona_index]], new_mexico_index))))

# Define row indices for Montana, South Dakota, and North Dakota
montana_index <- 24L  # Replace X with Montana's row index
south_dakota_index <- 39L  # Replace Y with South Dakota's row index
north_dakota_index <- 32L # Replace Z with North Dakota's row index

# Update the neighbors list
neighbors[[montana_index]] <- sort(unique(as.integer(c(neighbors[[montana_index]], south_dakota_index, north_dakota_index))))
neighbors[[south_dakota_index]] <- sort(unique(as.integer(c(neighbors[[south_dakota_index]], montana_index, north_dakota_index))))
neighbors[[north_dakota_index]] <- sort(unique(as.integer(c(neighbors[[north_dakota_index]], montana_index, south_dakota_index))))

# Update the neighbors list
neighbors[[wyoming_index]] <- sort(unique(as.integer(c(neighbors[[wyoming_index]], south_dakota_index))))
neighbors[[south_dakota_index]] <- sort(unique(as.integer(c(neighbors[[south_dakota_index]], wyoming_index))))

# Define row indices for Kentucky and Virginia
kentucky_index <- 15L  # Replace X with Kentucky's row index
virginia_index <- 44L  # Replace Y with Virginia's row index

# Update the neighbors list
neighbors[[kentucky_index]] <- sort(unique(as.integer(c(neighbors[[kentucky_index]], virginia_index))))
neighbors[[virginia_index]] <- sort(unique(as.integer(c(neighbors[[virginia_index]], kentucky_index))))

# Define row indices for Tennessee and North Carolina
tennessee_index <- 40  # Replace Z with Tennessee's row index
north_carolina_index <- 31L  # Replace A with North Carolina's row index

# Update the neighbors list
neighbors[[tennessee_index]] <- sort(unique(as.integer(c(neighbors[[tennessee_index]], north_carolina_index))))
neighbors[[north_carolina_index]] <- sort(unique(as.integer(c(neighbors[[north_carolina_index]], tennessee_index))))

# Update the neighbors list
neighbors[[utah_index]] <- sort(unique(as.integer(c(neighbors[[utah_index]], arizona_index))))
neighbors[[arizona_index]] <- sort(unique(as.integer(c(neighbors[[arizona_index]], utah_index))))

# Update the neighbors list
neighbors[[arkansas_index]] <- sort(unique(as.integer(c(neighbors[[arkansas_index]], tennessee_index))))
neighbors[[tennessee_index]] <- sort(unique(as.integer(c(neighbors[[tennessee_index]], arkansas_index))))

# Define row indices for Missouri and Kentucky
missouri_index <- 23L  # Replace Z with Missouri's row index

# Update the neighbors list
neighbors[[missouri_index]] <- sort(unique(as.integer(c(neighbors[[missouri_index]], kentucky_index))))
neighbors[[kentucky_index]] <- sort(unique(as.integer(c(neighbors[[kentucky_index]], missouri_index))))

# Define row indices for Michigan and Indiana
michigan_index <- 20L  # Replace X with Michigan's row index
indiana_index <- 12L  # Replace Y with Indiana's row index

# Update the neighbors list
neighbors[[michigan_index]] <- sort(unique(as.integer(c(neighbors[[michigan_index]], indiana_index))))
neighbors[[indiana_index]] <- sort(unique(as.integer(c(neighbors[[indiana_index]], michigan_index))))

# Define row indices for Michigan and Illinois
illinois_index <- 11L  # Replace Y with Illinois's row index

# Update the neighbors list
neighbors[[michigan_index]] <- sort(unique(as.integer(c(neighbors[[michigan_index]], illinois_index))))
neighbors[[illinois_index]] <- sort(unique(as.integer(c(neighbors[[illinois_index]], michigan_index))))

iowa_index <- 13L  # Replace Y with Iowa's row index

# Update the neighbors list
neighbors[[south_dakota_index]] <- sort(unique(as.integer(c(neighbors[[south_dakota_index]], iowa_index))))
neighbors[[iowa_index]] <- sort(unique(as.integer(c(neighbors[[iowa_index]], south_dakota_index))))

# Define row indices for Nevada, Arizona, and Oregon
oregon_index <- 35L  # Replace Z with Oregon's row index

# Update the neighbors list
neighbors[[arizona_index]] <- sort(unique(as.integer(c(neighbors[[arizona_index]], nevada_index))))  # Arizona only neighbors with Nevada
neighbors[[nevada_index]] <- sort(unique(as.integer(c(neighbors[[nevada_index]], arizona_index, oregon_index))))  # Nevada neighbors with Arizona & Oregon
neighbors[[oregon_index]] <- sort(unique(as.integer(c(neighbors[[oregon_index]], nevada_index))))  # Oregon neighbors with Nevada





lw <- nb2listw(neighbors, style = "B", zero.policy = TRUE)  # Use binary weights first

# Inverse distance weighting
for (i in 1:length(neighbors)) {
  neighbor_indices <- neighbors[[i]]
  total_weight <- 0
  for (j in 1:length(neighbor_indices)) {
    distance <- dist_matrix[i, neighbor_indices[j]]
    if (!is.na(distance) && distance != 0) {
      weight <- 1 / distance
      lw$weights[[i]][j] <- weight
      total_weight <- total_weight + weight
    } else {
      lw$weights[[i]][j] <- 0
    }
  }
  # Normalize weights
  if (total_weight != 0) {
    lw$weights[[i]] <- lw$weights[[i]] / total_weight
  }
}

# Function to replicate weights and neighbors while maintaining class attributes and style (NOT USED FOR SPATIAL PLM)
replicate_listw <- function(lw, times) {
  replicated_weights <- rep(lw$weights, times)
  replicated_neighbours <- do.call("c", replicate(times, lw$neighbours, simplify = FALSE))
  
  # Ensure neighbours has correct nb structure and region.id is a character
  class(replicated_neighbours) <- class(lw$neighbours)
  
  # create listw structure, copying the structure of lw
  expanded_weights <- list(
    style = lw$style,  # Place style first
    neighbours = replicated_neighbours,  # Place neighbours second
    weights = replicated_weights  # Place weights last
  )
  class(expanded_weights) <- class(lw)
  
  return(expanded_weights)
}

# Determine the number of years
num_years <- length(unique(final_df$Year))
num_rows <- nrow(final_df)
num_reps <- num_rows / length(lw$weights)

weights_across_years <- replicate_listw(lw, num_reps)

# Plot neighbors connections
plot(st_geometry(nb), border = "black") # plot polygons with gray borders
plot(neighbors, st_coordinates(st_centroid(st_geometry(nb))), add = TRUE, col = "blue", lwd = 1.5)
```

```{r}
#neighbor exploration
neighbors[[30]] 
model_ready$State[c(6  ,7 ,18 ,19 ,27 ,28 ,36 ,37 ,43)] # adjust based on neighbors[[i]]

#weights exploration
lw$weights[[30]]
test <-lw$weights[[30]]
sum(test)
```

```{r}
#neighbor exploration
neighbors[[1]] 
model_ready$State[c(8  ,9 ,16 ,22 ,40)] # adjust based on neighbors[[i]]

#weights exploration
lw$weights[[1]]
```

# Correlations and Finding The Best Functional Forms 

Below are the variables I’ve decided to include in my model so far. `home_damage` is my treatment variable, and `homePrems` is my response variable. I believe the risk of natural disasters should be controlled for in the model. While disaster damage isn’t the best measure of risk, controlling for the damage caused could serve as an indirect measure. Surprisingly, `disaster_damage` and `home_damage` are only slightly correlated (~0.20). My expectation is that both damage variables should have a positive effect on homeowners' premiums. The remaining explanatory variables are price deflator, median household income, and population density. `PriceDef` is an indirect measure of inflation and helps control for the economic status of each state relative to the national average price deflator. Median household income provides insight into the socioeconomic status of the median household and, in theory, reflects the quality of the median home—wealthier areas would theoretically have higher homeowners' premiums. Finally, I included population density to capture storm effectiveness. For example, if the same storm hits both Wyoming and New York, we would expect more damage in New York simply because it has a higher probability of impacting densely populated areas compared to Wyoming.

However, there’s an issue with how highly correlated the last three explanatory variables are. `PriceDef` and `MedianHomeIncome` have a correlation of about 0.80, which is definitely a problem I need to address. The challenge is that both variables seem necessary for controlling key factors in my model. One potential solution is to replace median household income with average home prices, but I suspect I’d run into the same correlation issue since both prices and incomes have been steadily increasing since 2008. If I drop either variable from the `splm` models, the results remain about the same. Population density is correlated at ~0.50 with the price-related variables, but I don’t see `PopDensity` as essential compared to the others, especially if I opt for a fixed-effects model. That said, dropping `PopDensity` won’t resolve the correlation issue between `PriceDef` and `MedianHomeIncome`.

```{r}
# Subset the relevant variables
vars <- st_drop_geometry(model_ready_scaled)[, c("homePrems", "home_damage", "disaster_damage", "MedHomeIncome", "PriceDef", "PopDensity", "Percent_white", "AvgHomePrice", "combined_damage", "climate_index", "building_risk", "realized_damaged", "actual_NRI")]

# Compute the correlation matrix
cor_matrix <- cor(vars, use = "pairwise.complete.obs")

# Create the correlation plot with numbers
corrplot(cor_matrix, method = "circle", type = "full", 
         addgrid.col = "black",  # Add a grid in black color
         tl.cex = 0.8,           # Adjust text label size
         tl.col = "black",       # Label color
         cl.cex = 0.8,           # Color legend size
         diag = TRUE,
         addCoef.col = "black",  # Add numerical values
         number.cex = 0.8)       # Adjust coefficient font size
```

```{r}
ggplot(model_ready, aes(Year, Percent_white)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +  # Use a single color for the regression line
  labs(
    title = "Average Homeowners Premiums vs PopDensity",
    x = "PopDensity",
    y = "Average Premiums") +
      facet_wrap(~State)
```


Below are graphs that show the best functional form of each variable.

# Linear

I started with a simple linear regression model that found a positive effect from home damage. Moran’s I test indicated evidence of spatial autocorrelation in the model. Additionally, `lm.LMtests()` showed that in this basic model, the lagged effect is stronger than the spatial errors.

Looking at a linear regression that includes all explanatory variables, the model still found a positive effect from home damage, but the effect of disaster damage was negative and quite insignificant. Moran’s I remains significant, but this time, `lm.LMtests()` indicated that the spatial error was more significant than the lag.
```{r}
lm2 <- lm(homePrems ~ log(home_damage) + log(disaster_damage) + AvgHomePrice + Percent_white  + log(PopDensity), data = model_ready_scaled)

summary(lm2)

### Moran's I ### which quantifies how similar each region is with its neighbors and averages all these assessments.
residuals <- residuals(lm2)

moran.test(residuals, weights_across_years, alternative = "greater")

MC <- moran.mc(residuals, weights_across_years, nsim = 999, alternative = "greater") # prefered method 

plot(MC)

LM_state <- lm.LMtests(lm2, weights_across_years, test = "all")
LM_state
```

# Fixed Effects

A big part of last week was deciding whether a fixed effects or random effects model would be more appropriate for what I want to model. Before answering that, let’s examine how each fits the data without spatial weights.  

Looking at a fixed effects model, the results are similar, but with higher standard errors. As a result, this model identifies fewer significant variables compared to the linear models. However, it did find an insignificant Moran’s I. Since I’m interested in spatial relationships, this model doesn’t correctly fit the data. Additionally, the `panel_lme()` and `panel_lml()` functions provided evidence of spatial lags and errors, with errors being more significant again. 
```{r}
### FIXED EFFECTS MODEL ###
fixed_effects_model <- plm(homePrems ~ log(home_damage)+ log(disaster_damage) + AvgHomePrice + Percent_white  + log(PopDensity), data = model_ready_scaled, index = c("State", "Year"), model = "within", effect = "twoways")

# Model Summary
summary(fixed_effects_model)

### Moran's I ###
# Extract residuals from the fixed effects model
residuals_fixed_effects <- residuals(fixed_effects_model)

fixed_test<- homePrems ~ log(home_damage) + log(disaster_damage) + PriceDef + log(PopDensity) + MedHomeIncome + I(MedHomeIncome^2) + Coastal
slmtest(fixed_test, model_ready_scaled, lw, test = c("rlme"), index = c("State", "Year"), model = "pooling") 
slmtest(fixed_test, model_ready_scaled, lw, test = c("rlml"), index = c("State", "Year"), model = "pooling") #"lme","lml","rlme","rlml"

```

# Mixed Effects Model

The basic mixed effects model is the first to find a negative effect from home damage while still producing a significant Moran’s I.

Even with the other explanatory variables added, the results differ from the previous models. I believe this is due to the assumptions made by mixed effects models, specifically that random effects are uncorrelated with the other explanatory variables. For this reason, my final model will include fixed effects rather than random effects. However, I also include spatial models with random effects to demonstrate that the specification does not significantly change the results when I account for spatial errors.

```{r}
long_df <- model_ready_scaled %>%
  mutate(Year_Long = Year - 2008)
# Fit the mixed effects model
mixedeffects <- lmer(homePrems ~ log(home_damage) + dplyr::lag(log(home_damage)) + log(disaster_damage) + dplyr::lag(log(disaster_damage)) + AvgHomePrice + Percent_white  + log(PopDensity) + (Year_Long|State) , REML = TRUE, data = long_df)

# Summary of the model
summary(mixedeffects)
```

# Lagsarlm and Errorsarlm Models

I did these models based of the these two articles you sent me:

https://crd230.github.io/lab8.html#Lagrange_Multiplier_test & https://www.emilyburchfield.org/courses/gsa/spatial_regression_lab. 

Surprisingly, both models produced similar results. Both produced significant spatial parameters(lambda and rho). To me this shows that there is a need to account for both spatial lag and error, however it appears to be frowned upon to account for both. So based on the majority of the previous tests, I will stick with a spatial error model and put spatial lag models and a full spatial lag and error model (SARMA) in the appendix. 

```{}
lag_model <- lagsarlm(homePrems ~ log(home_damage) + log(disaster_damage) + AvgHomePrice + Percent_white + log(PopDensity) + as.factor(State), data = model_ready_scaled,  listw = listw2U(weights_across_years), zero.policy = TRUE, type = "mixed")

summary(lag_model)

#fit.lag.effects <- impacts(lag_model, listw = listw2U(weights_across_years), R = 999)
#summary(fit.lag.effects, zstats = TRUE, short = TRUE)
```

```{}
fit.err <- errorsarlm(homePrems ~ log(home_damage) + log(disaster_damage) + AvgHomePrice + Percent_white + log(PopDensity) + as.factor(State), data = model_ready_scaled,  listw = listw2U(weights_across_years), zero.policy = TRUE, method = "Matrix")

summary(fit.err)
```



# Spatial Panel Models

As stated earlier, I include both random effects and fixed effects to show that the results are robust to this specification choice. Additionally, I use `sphtest()` to further justify that fixed effects better fit the data. Based on this https://libguides.princeton.edu/R-Panel?utm_source=chatgpt.com, the significant p-values indicate that fixed effects are the better choice in this case.  

The four models below demonstrate consistent results, regardless of whether random or fixed effects are used. Aside from the mixed effects models, most models show a positive effect for damage covered by homeowners insurance and a negative effect for damage not covered. I’m not yet sure if the latter result makes sense, but the former is an exciting finding at this stage. When I remove disaster damage from the model, I still find a significant positive effect from `home_damage`.  

As I dive deeper into the literature on the relationship between homeowners' premiums and weather-related damages, I aim to refine my reasoning and explore new control variables for my model. If my understanding of the models is correct, I believe I’m in a strong position to fine-tune the analysis effectively. 


```{r}
panel_data <- model_ready 

panel_data <- panel_data %>%
  mutate(across(where(is.numeric), ~ ifelse(. == 0, . + 1e-07, .)))



# Scale variables
panel_data_scaled <- panel_data %>%
  mutate(across(c(homePrems, rentersPrems, HealthcareExp, Pop18_64, Uninsured, Medicaid, PriceDef, disaster_damage, PopDensity, Total_Residents, td_per_cap, MedHomeIncome, home_damage, Percent_white, AvgHomePrice, ExpAnnualLoss, RiskIndex, Total_Buidling_Value, combined_damage, climate_index, building_risk, realized_damaged, actual_NRI), 
                ~ {
                  scaled_value <- (.-min(.)) / (max(.) - min(.))
                  scaled_value[scaled_value == 0] <- 0.00000000000001
                  scaled_value
                }))
```

```{r}
# Subset the relevant variables
vars <- st_drop_geometry(model_ready_scaled)[, c("homePrems", "home_damage", "disaster_damage", "combined_damage", "MedHomeIncome", "PriceDef", "PopDensity", "Percent_white", "AvgHomePrice", "climate_index", "building_risk", "realized_damaged", "actual_NRI")]

# Compute the correlation matrix
cor_matrix <- cor(vars, use = "pairwise.complete.obs")

# Create the correlation plot with numbers
corrplot(cor_matrix, method = "circle", type = "full", 
         addgrid.col = "black",  # Add a grid in black color
         tl.cex = 0.8,           # Adjust text label size
         tl.col = "black",       # Label color
         cl.cex = 0.8,           # Color legend size
         diag = TRUE,
         addCoef.col = "black",  # Add numerical values
         number.cex = 0.8)       # Adjust coefficient font size
```

```{}
# Aggregate the total damage by state while keeping geometry
state_damage <- model_ready %>%
  group_by(State) %>%
  summarise(homePrems = mean(homePrems, na.rm = TRUE),
            geometry = st_union(geometry)) %>%
  ungroup()

# Create a leaflet map
leaflet(state_damage) %>%
  addTiles() %>%  # Add base tiles
  addPolygons(fillColor = ~colorNumeric("YlOrRd", homePrems)(homePrems),
              color = "black", weight = 1, fillOpacity = 0.7,
              popup = ~paste(State, "<br> Homeowner Prems: ", homePrems)) %>%
  addLegend("bottomright", 
            pal = colorNumeric("YlOrRd", state_damage$homePrems, n = 8),
            values = state_damage$homePrems,
            title = "Homeowner Prems",
            opacity = 1)
```

```{}
# Aggregate the total damage by state while keeping geometry
state_damage <- model_ready %>%
  group_by(State) %>%
  summarise(Percent_white_mean = mean(Percent_white, na.rm = TRUE),
            geometry = st_union(geometry)) %>%
  ungroup()

# Create a leaflet map
leaflet(state_damage) %>%
  addTiles() %>%  # Add base tiles
  addPolygons(fillColor = ~colorNumeric("YlOrRd", Percent_white_mean)(Percent_white_mean),
              color = "black", weight = 1, fillOpacity = 0.7,
              popup = ~paste(State, "<br> Total Damage: ", Percent_white_mean)) %>%
  addLegend("bottomright", 
            pal = colorNumeric("YlOrRd", state_damage$Percent_white_mean, n = 8),
            values = state_damage$Percent_white_mean,
            title = "Percent White",
            opacity = 1)
```

```{r}
# Aggregate the total damage by state while keeping geometry
state_damage_2 <- model_ready %>%
  group_by(State) %>%
  summarise(home_damage = sum(home_damage, na.rm = TRUE), 
            disaster_damage = sum(disaster_damage, na.rm = TRUE),
            geometry = st_union(geometry)) %>%
  ungroup()

# Create a leaflet map
leaflet(state_damage_2) %>%
  addTiles() %>%  # Add base tiles
  addPolygons(fillColor = ~colorQuantile("YlOrRd", home_damage)(home_damage),
              color = "black", weight = 1, fillOpacity = 0.7,
              popup = ~paste(State, "<br> Total Damage: ", home_damage)) %>%
  addLegend("bottomright", 
            pal = colorNumeric("YlOrRd", state_damage_2$home_damage, n = 8),
            values = state_damage_2$home_damage,
            title = "Total Home Damage",
            opacity = 1)

# Create a leaflet map
leaflet(state_damage_2) %>%
  addTiles() %>%  # Add base tiles
  addPolygons(fillColor = ~colorNumeric("YlOrRd", disaster_damage)(disaster_damage),
              color = "black", weight = 1, fillOpacity = 0.7,
              popup = ~paste(State, "<br> Total Damage: ", disaster_damage)) %>%
  addLegend("bottomright", 
            pal = colorNumeric("YlOrRd", state_damage_2$disaster_damage, n = 8),
            values = state_damage_2$disaster_damage,
            title = "Total Diaster Damage",
            opacity = 1)
```


## Model 1

###############################################################

```{r}
# testing some shit
## Final Model
# Error
M1S <- spml(homePrems ~ log(home_damage) + log(disaster_damage) + AvgHomePrice + Percent_white + log(PopDensity),
                   data = panel_data, 
                   index = c("State", "Year"),
                   listw = lw,
                   model = "within",
                   effect = "individual",
                   lag = FALSE,
                   spatial.error = "b")

summary_M1S <- summary(M1S)

summary_M1S

model1 <- coeftest(summary_M1S)
```

```{r}
## Final Model
# Error
M1Y <- spml(homePrems ~ log(home_damage) + log(disaster_damage) + AvgHomePrice + Percent_white + log(PopDensity) + Region,
                   data = panel_data, 
                   index = c("State", "Year"),
                   listw = lw,
                   model = "within",
                   effect = "time",
                   lag = FALSE,
                   spatial.error = "b")

summary_M1Y <- summary(M1Y)

summary_M1Y

model2 <- coeftest(summary_M1Y)
```

```{r}
## Final Model
# Error
M1R <- spml(homePrems ~ log(home_damage) + log(disaster_damage) + AvgHomePrice + Percent_white + log(PopDensity) + Region,
                   data = panel_data, 
                   index = c("State", "Year"),
                   listw = lw,
                   model = "random",
                   effect = "individual",
                   lag = FALSE,
                   spatial.error = "b")
summary(M1R)
```

##############
```{r}
# Lag
M2S <- spml(homePrems ~ log(actual_NRI) + lag(log(actual_NRI)) + AvgHomePrice + Percent_white + log(PopDensity),
                   data = panel_data_scaled, 
                   index = c("State", "Year"),
                   listw = lw,
                   model = "within",
                   effect = "individual",
                   lag = FALSE,
                   spatial.error = "b")

summary_M2S <- summary(M2S)

summary_M2S

model3 <- coeftest(summary_M2S)
```

```{r}
# Lag
M2Y <- spml(homePrems ~ log(actual_NRI) + lag(log(actual_NRI)) + AvgHomePrice + Percent_white + log(PopDensity),
                   data = panel_data_scaled, 
                   index = c("State", "Year"),
                   listw = lw,
                   model = "within",
                   effect = "time",
                   lag = FALSE,
                   spatial.error = "b")

summary_M2Y <- summary(M2Y)

summary_M2Y

model4 <- coeftest(summary_M2Y)
```

```{r}
# random
M2R <- spml(homePrems ~ log(actual_NRI) + lag(log(actual_NRI)) + AvgHomePrice + Percent_white + log(PopDensity),
                   data = panel_data_scaled, 
                   index = c("State", "Year"),
                   listw = lw,
                   model = "random",
                   effect = "individual",
                   lag = FALSE,
                   spatial.error = "b")
summary(M2R)
```
#########
```{r}
## Final Model
# lag
M3S <- spml(homePrems ~ log(home_damage) + lag(log(home_damage)) + log(disaster_damage) + lag(log(disaster_damage)) +  log(actual_NRI) + lag(log(actual_NRI)) + AvgHomePrice + Percent_white + log(PopDensity),
                   data = panel_data_scaled, 
                   index = c("State", "Year"),
                   listw = lw,
                   model = "within",
                   effect = "individual",
                   lag = FALSE,
                   spatial.error = "b")

summary_M3S <- summary(M3S)

summary_M3S

model5 <- coeftest(summary_M3S)
```

```{r}
## Final Model
M3Y <- spml(homePrems ~ log(home_damage) + lag(log(home_damage)) + log(disaster_damage) + lag(log(disaster_damage)) + log(actual_NRI) + lag(log(actual_NRI)) + AvgHomePrice + Percent_white  + log(PopDensity),
                   data = panel_data_scaled, 
                   index = c("State", "Year"),
                   listw = lw,
                   model = "within",
                   effect = "time",
                   lag = FALSE,
                   spatial.error = "b")

summary_M3Y <- summary(M3Y)

summary_M3Y

model6 <- coeftest(summary_M3Y)
```

```{r}
## Final Model
M3R <- spml(homePrems ~ log(home_damage) + lag(log(home_damage)) + log(disaster_damage) + lag(log(disaster_damage)) + log(actual_NRI) + lag(log(actual_NRI)) + AvgHomePrice + Percent_white  + log(PopDensity),
                   data = panel_data_scaled, 
                   index = c("State", "Year"),
                   listw = lw,
                   model = "random",
                   effect = "individual",
                   lag = FALSE,
                   spatial.error = "b")

summary_M3R <- summary(M3R)

summary_M3R
```

```{r}
sphtest(M1S,M1R)
sphtest(M1Y,M1R)
sphtest(M2S,M2R)
sphtest(M2Y,M2R)
sphtest(M3S,M3R)
sphtest(M3Y,M3R)
```

```{r, warning=FALSE, message=FALSE, results= "asis", include=TRUE}
stargazer(model1,model2,model3,model4,model5,model6,
          type = "text",
          report=('vc*p'),
          single.row = TRUE,
          digits = 3,
          keep.stat = c("n","rsq","adj.rsq"),
          notes = "P-values reported in parentheses, <em>&#42;p&lt;0.1;&#42;&#42;p&lt;0.05;&#42;&#42;&#42;p&lt;0.01</em>",
          notes.append = FALSE,
          title = "Regression Models",
          model.numbers = FALSE,
          column.labels = c("State FE", "Year FE", "State FE", "Year FE", "State FE", "Year FE"))
```

#######################################################################################################################

# paper graphics 
```{r}
# Subset the relevant variables
vars <- st_drop_geometry(model_ready_scaled)[, c("homePrems", "home_damage", "disaster_damage", "PriceDef", "PopDensity", "Percent_white", "AvgHomePrice", "actual_NRI")]

# Compute the correlation matrix
cor_matrix <- cor(vars, use = "pairwise.complete.obs")

# Create the correlation plot with numbers
corrplot(cor_matrix, method = "circle", type = "full", 
         addgrid.col = "black",  # Add a grid in black color
         tl.cex = 0.8,           # Adjust text label size
         tl.col = "black",       # Label color
         cl.cex = 0.8,           # Color legend size
         diag = TRUE,
         addCoef.col = "black",  # Add numerical values
         number.cex = 0.8)       # Adjust coefficient font size
```

```{r}
selected_vars <- model_ready[, c("homePrems", "home_damage", "disaster_damage", 
                        "actual_NRI", "AvgHomePrice", "Percent_white", 
                        "PopDensity")]

stargazer(as.data.frame(selected_vars), 
          type = "text",
          summary.stat = c("n", "mean", "sd", "min", "p25", "median", "p75", "max"),
          title = "Summary Statistics")  

```

$$
\begin{aligned}
\text{homePrems}_{i} &= \rho \sum_{j=1}^{n} W_{ij} y_{j} + \beta_0 + \beta_1 \log(\text{home damage}_{i}) + \beta_2 lag(\log(\text{home damage}_{i})) \\ 
&\quad + \beta_3 \log(\text{disaster damage}_{i}) + \beta_4 lag(\log(\text{disaster damage}_{i})) \\ 
 
&\quad + \beta_5 \text{AvgHomePrice}_{i} + \beta_6 \text{Percent white}_{i} + \beta_7 \log(\text{PopDensity}_{i}) + \alpha_i + \epsilon_{i} \\
\text{where } \epsilon_{i} &\sim \rho W_i y_i + u_{i}
\end{aligned}
$$

$$
\begin{aligned}
\text{homePrems}_{t} &= \rho \sum_{j=1}^{n} W_{jt} y_{j} + \beta_0 + \beta_1 \log(\text{home damage}_{t}) + \beta_2 lag(\log(\text{home damage}_{t})) \\ 
&\quad + \beta_3 \log(\text{disaster damage}_{t}) + \beta_4 lag(\log(\text{disaster damage}_{t})) \\ 
&\quad + \beta_5 \text{AvgHomePrice}_{t} + \beta_6 \text{Percent white}_{t} + \beta_7 \log(\text{PopDensity}_{t}) + \alpha_t + u_{t} \\
\end{aligned}
$$
$$
\begin{aligned}
\text{homePrems}_{i} &= \rho \sum_{j=1}^{n} W_{ij} y_{j} + \beta_0 + \beta_1 \log(\text{actual NRI}_{i}) + \beta_2 lag(\log(\text{actual NRI}_{i}))
 \\ + &\quad \beta_3 \text{AvgHomePrice}_{i} + \beta_4 \text{Percent white}_{i} + \beta_5 \log(\text{PopDensity}_{i}) + \alpha_i + u_{i} \\
\end{aligned}
$$

```{r}
library(knitr)
library(kableExtra)

# Function to round numeric columns to 2 decimal places
round_df <- function(df, digits) {
  df %>% mutate(across(where(is.numeric), ~ round(., digits)))
}

# Assume your data frame is named 'model_ready'
# Extract the variables of interest
vars_of_interest <- model_ready[, c("State" , "Year", "homePrems", "home_damage", "disaster_damage", "AvgHomePrice", "Percent_white", "PopDensity")]

vars_of_interest <- vars_of_interest %>%
  rename(
    "Home Premiums" = homePrems,
    "Home Damage" = home_damage,
    "Disaster Damage" = disaster_damage,
    "Percent White" = Percent_white,
    "Avg Home Price" = AvgHomePrice
  )

# Round the numeric columns to 2 decimal places
vars_of_interest <- round_df(vars_of_interest, 2)

# Show the first 5 rows
table <- head(vars_of_interest, 5)

# Create a professional-looking table with vertical lines separating each variable
kable(table, format = "html", table.attr = "style='width:100%;'") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE) %>%
  add_header_above(c(" " = 1, "Table 1: Selected Variables from the first 5 rows in the dataset" = 8)) %>%  # Adds a header above the table
  column_spec(1, border_right = TRUE) %>%  # Adds vertical lines to the first column
  column_spec(2, border_right = TRUE) %>%  # Adds vertical lines to the second column
  column_spec(3, border_right = TRUE) %>%  # Adds vertical lines to the third column
  column_spec(4, border_right = TRUE) %>%  # Adds vertical lines to the fourth column
  column_spec(5, border_right = TRUE) %>%  # Adds vertical lines to the fifth column
  column_spec(6, border_right = TRUE) %>%  # Adds vertical lines to the sixth column
  column_spec(7, border_right = TRUE) %>%   # Adds vertical lines to the seventh column
  column_spec(8, border_right = TRUE)

```




# Appendix 

## Spatial Lag Models

The four splm models but now as a spatial lag model. The signs for the coeffiecents are still the same (besides the full random effect model), however the standard errors have changed, drastically in some of the models. 

```{r}
# spatial mixed effects model, base
BMR <- spml(homePrems ~ log(home_damage), 
           data = panel_data_scaled, 
           index = c("State", "Year"),
           listw = lw,
           model = "random",
           effect = "individual",
           lag = TRUE,
           spatial.error=c("none"))

summary(BMR)
```

```{r}
# Spatial fixed effects model with both State and Year fixed effects
BMF <- spml(homePrems ~ log(home_damage), 
                   data = panel_data_scaled, 
                   index = c("State", "Year"),
                   listw = lw,
                   model = "within",
                   effect = "twoways",
                   lag = TRUE,
                   spatial.error = "none")

summary(BMF)
```


```{r}
# spatial mixed effects model, homeowners
FMR <- spml(homePrems ~ log(home_damage) + log(disaster_damage) + PriceDef + log(PopDensity) + MedHomeIncome + I(MedHomeIncome^2) + Percent_white + log(AvgHomePrice),
           data = panel_data_scaled, 
           index = c("State", "Year"),
           listw = lw,
           model = "random",
           effect = "individual",
           lag = FALSE,
           spatial.error=c("none"))

summary(FMR)
```

```{r}
# spatial mixed effects model, homeowners
FMF <- spml(homePrems ~ log(home_damage) + log(disaster_damage) + PriceDef + log(PopDensity) + MedHomeIncome + I(MedHomeIncome^2) + Percent_white + log(AvgHomePrice),
                   data = panel_data_scaled, 
                   index = c("State", "Year"),
                   listw = lw,
                   model = "within",
                   effect = "twoways",
                   lag = TRUE,
                   spatial.error = "none")
summary(FMF)
```

```{r}
#If p-value is significant, use a fixed effects model

sphtest(FMF,FMR)
```

## SARMA MODEL

I do think this model should be looked at more, the error term (rho) is suprisingly insignificant while the lagged term (lambda) is significant. Why this model produces this result is not clear to me, but it is interesting that the coefficients produce the similar results to the rest of the models. 

```{r}
# spatial mixed effects model, homeowners
FMF <- spml(homePrems ~ log(home_damage) + log(disaster_damage) + PriceDef + log(PopDensity) + MedHomeIncome + I(MedHomeIncome^2) + Percent_white + log(AvgHomePrice),
                   data = panel_data_scaled, 
                   index = c("State", "Year"),
                   listw = lw,
                   model = "within",
                   effect = "twoways",
                   lag = TRUE,
                   spatial.error = "b")
summary(FMF)
```